{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaaa211",
   "metadata": {},
   "source": [
    "### <center> \"Sentiment Analysis with Python and NLP using Amazon Software Reviews dataset\" </center>\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001d226",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "\n",
    "[Abstract](#0-bullet)\n",
    "\n",
    "[1. Business Understanding](#1-bullet)\n",
    "\n",
    "[2. Data Understanding](#2-bullet)\n",
    "  \n",
    "[3. Data Preparation](#3-bullet)\n",
    "\n",
    "[4. Modeling](#4-bullet)\n",
    "\n",
    "[5. Evaluation](#5-bullet)\n",
    "\n",
    "[6. Discuss and Conclusion](#6-bullet)\n",
    "\n",
    "[7. Bibliography](#7-bullet)\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b5750",
   "metadata": {},
   "source": [
    "### Abstract <a class=\"anchor\" id=\"0-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d22e80",
   "metadata": {},
   "source": [
    "This project focuses on customer sentiment analysis using Amazon Review Data on Software products to understand customer feelings about the brands and products through their reviews. The project is implemented based on the CRISP-DM model through 5 phases, including Business Understanding, Data Understanding, Data Preparation, Modeling, and Evaluation. All the phases will be implemented using NLTK and Scikit-learn in Python.\n",
    "\n",
    "In the Data Understanding phase, Data Insights are exposed in Exploratory Data Analysis (EDA) step. Next, Natural Language Processing (NLP) technique is used to tokenize text data and generate the Word Cloud in the Data Preparation phase. In addition to that, advanced text processing techniques like Normalization and Lemmatization are used to clean and normalize data in this project. Before modeling, Review text data is vectorized into a numerical representation sparse matrix using the Bag of Words executed by the Term Frequency Inverse Document Frequency (TF-IDF) method. Linear Support Vector Classification (LinearSVC) is used to predict categorical rating class due to good performance in sparse data such as text data. LinearSVC is the main algorithm for modeling. However, LogisticRegression is also used for performance reference.\n",
    "\n",
    "In our project, four notebooks are created to perform all CRISP-DM phases: \n",
    "- **01. Main Notebook** : Main report, Business Objectives, Data Mining goals and the summary of the project\n",
    "- **02. Data Understanding** : Data Collection, Data Wrangling, EDA\n",
    "- **03. Data Preparation** : Data Cleaning, Data Normalization, WordCloud\n",
    "- **04. Modeling and Evaluation** : Data Preprocessing, Modeling and Evaluation\n",
    "\n",
    "Four notebooks are organized and developed in order. The Output of phase 02 will be input of phase 03. The output of phase 03 will be input of phase 04. Therefore, **please kindly run them in the right order arranged**.\n",
    "\n",
    "Project folder structure:\n",
    "\n",
    "**Project_TanThienNguyen** : root-folder: contains all .ipynb files\n",
    "\n",
    "**/Dataset** sub-folder: store original dataset (meta_Software.json and Software.json)\n",
    "\n",
    "**/Output** sub-folder: store output of phases (reviewEDA.json and reviewPreprocessing.json)\n",
    "\n",
    "**/HTML exported Notebooks** sub-folder: store all exported notebooks in HTML format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f8ce5",
   "metadata": {},
   "source": [
    "### 1. Business Understanding <a class=\"anchor\" id=\"1-bullet\"></a>\n",
    "#### 1.1. Business Analysis\n",
    "Customer Sentiment Analysis is one of the key factors affecting the success of e-commerce companies. There are many ways to analyse customer sentiment, including surveys, chatbots, using call center data, and especially, analysing reviews and ratings. Using text analytics and classification to make sense of customer reviews will help companies improve customer service and attract more clients.\n",
    "\n",
    "#### 1.2. Business Objectives\n",
    "- Understand customer feelings about the brand and products to improve the quality of services and brand reputation\n",
    "- Understand the market trend through customer reviews to optimize marketing strategies\n",
    "\n",
    "#### 1.3 Data Mining Goals\n",
    "- Using NLP to generate the word cloud and find the most frequent words in reviews\n",
    "- Building a machine learning model using the LinearSVC algorithm to predict categorical customer ratings based on customer reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f670ed",
   "metadata": {},
   "source": [
    "### 2. Data Understanding <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    "#### 2.1. Data Collection\n",
    "The datasets include Amazon Software product reviews and metadata Software products collected from https://jmcauley.ucsd.edu/data/amazon/ or http://snap.stanford.edu/data/web-Amazon.html. Accessed date: 25 January 2022.\n",
    "\n",
    "The datasets then were downloaded and stored in the **/Dataset** local folder\n",
    "\n",
    "#### 2.2. Data Description\n",
    "The datasets includes two files:\n",
    "- **Software.json** - Amazon Software product reviews includes 459,436 reviews in JSON format (300 MB)\n",
    "- **meta_Software.json** - meta data includes meta data of 26,815 products in JSON format (70 MB)\n",
    "\n",
    "#### 2.3. Data Wrangling\n",
    "##### This Data Wrangling part will do the followings:\n",
    "1. Drop unnecessary columns and transform some columns into two raw datasets. In more detail:\n",
    "2. Find and remove duplicated rows based on asin, reviewFull, reviewTime, and reviewerID in the Product Review data set\n",
    "3. **vote** with NaN value will be transformed to numerical value = 0\n",
    "4. **reviewTime** will be transformed to Date type\n",
    "5. Find and remove duplicated rows based on asin in the Product metadata\n",
    "6. Remove special characters\n",
    "7. Update product category\n",
    "8. Merge two datasets into a new one using asin (product key)\n",
    "\n",
    "#### 2.4. Exploratory Data Analysis\n",
    "##### In this part, we will explore the below information:\n",
    "- The number of reviews by rating\n",
    "- The number of votes by rating\n",
    "- The number of reviews by year\n",
    "- Top 10 most reviewed brands, categories, products\n",
    "- Top 10 most 5-star rated brands, categories, products\n",
    "- The correlation between rating, vote, and reviewLength\n",
    "\n",
    "#### 2.5. Verify Data Quality\n",
    "In this project, we focus on reviewText and rating for rating classification prediction. Therefore, in the last step of the Data Understanding phase, we will verify the data quality of these columns.\n",
    "- Verify missing and null reviewText values\n",
    "- Verify missing and null rating values\n",
    "\n",
    "Data then is exported to the **Output/reviewEDA.json** file\n",
    "\n",
    "### Link to [02.Data Understanding Notebook](CA_TanThienNguyen_C00278719_02_DataUnderstanding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1ebd5",
   "metadata": {},
   "source": [
    "### 3. Data Preparation   <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "#### 3.1. Select data\n",
    "The dataset exported from the EDA phase will be loaded to preprocess here before going to the modeling phase. Only features needed for the modeling phase are selected.\n",
    "\n",
    "In this project, we focus on two data mining goals which are Using NLP to generate the word cloud and find the most frequent words in reviews and Building a machine learning model using the SVM algorithm to predict categorical customer ratings. \n",
    "\n",
    "Therefore, we will select only reviewText and rating features for this phase.\n",
    "\n",
    "#### 3.2. Clean Data\n",
    "Data Cleaning is a must-have step in all machine learning projects. It helps us to avoid Garbage In, Garbage Out (GIGO). In detail, it means that the quality of the output is determined by the quality of the input. Therefore, Cleaning data is so important, and the Data Preparation phase usually takes 70-80% workload of a project (IBM Corporation, 2011). This phase will apply the following methods to clean reviewText:\n",
    "- Remove html tags and special characters\n",
    "- Remove punctuation\n",
    "- Remove stopwords\n",
    "- Lowercase \n",
    "- Lemmatization\n",
    "\n",
    "**Normalization** includes some cleaning steps:\n",
    "\n",
    "- Removing html tags, punctuation, and special characters in text data such as '&@#?nbsp;' or '<a href'. \n",
    "- Removing stopwords that are too frequent in documents but not informative and meaningful. Lowercase text data to minimize the number of word variants.\n",
    "\n",
    "**Stemming and Lemmatization** are two of the advanced normalization forms. However, stemming just drops common suffixes based on a dictionary of known word forms, e.g., play, plays, playing, played will be retreated as play. It may lead to missing meaningful words or even erasing the words, e.g., was will be trimmed to wa. Lemmatization considers the role of the word in the sentence and produces better results than stemming when used for text normalization for machine learning (Andrea and Sarah, 2017).\n",
    "\n",
    "#### 3.3. Construct and Format Data\n",
    "Our second Data mining goal is to predict categorical customer ratings based on customer reviews. We would like to know whether a review is good or bad.\n",
    "\n",
    "Therefore, We will add ratingClass column into the dataset by dividing rating into two categories: Good (rating >=4) and Bad (rating < 4)\n",
    "\n",
    "ratingClass will be our target variable for predicting rating classification based on reviews.\n",
    "\n",
    "#### 3.4. Generate WordCloud\n",
    "Our first Data Mining goal is to use NLP to generate the word cloud and find the most frequent words in reviews.\n",
    "\n",
    "Word Cloud will be generated for all text data, for good rating class, and for bad rating class.\n",
    "\n",
    "Data then is exported to the **Output/reviewPreprocessing.json** file\n",
    "\n",
    "### Link to detail [03.Data Preparation Notebook](CA_TanThienNguyen_C00278719_03_DataPreparation.ipynb) \n",
    "(Note: Please remember to run **02.Data Understanding Notebook** first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1d44",
   "metadata": {},
   "source": [
    "### 4. Modeling    <a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "#### 4.1. Data Preprocessing\n",
    "Basically, in the CRISP-DM model design, This part will be implemented in phase 03. Data Preparation. However, to make it easy to follow up and do necessary tests, I move the part Data Preprocessing to phase 04. Modeling in this notebook. This part will do the followings:\n",
    "\n",
    "- **Feature Selection**\n",
    "\n",
    "Only necessary features used in modeling will be selected. In particular, the cleanText feature and target variable, ratingClass are selected. We will go into detail about Target variable descriptive statistics and encode it.\n",
    "\n",
    "- **Split train set and test set**\n",
    "\n",
    "We will use the train_test_split transformer in the scikit-learn library to split the training set and test set with the ratio of 75:25 and stratified sampling.\n",
    "\n",
    "We do this step first so that we can ensure the test set is as unbiased as it can be and reflects a true evaluation of the model. If we balance data before splitting might introduce bias in the test set where a few data points in the test set are synthetically generated and well-known from the training set.\n",
    "\n",
    "#### 4.2. Feature Vectorizing - Bags of Words (TF-IDF) \n",
    "\n",
    "Most algorithms require numerical features, so we need to transform the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. Therefore, we implement the vectorization process to turn a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting, and normalization) is called the ***Bag of Words or “Bag of n-grams”*** representation (Andrea and Sarah, 2017, Scikit-learn Developers, 2022).\n",
    "\n",
    "To overcome this, we use ***TfidfTransformer- Term Frequency Inverse Document Frequency (TF-IDF)*** to implement Bag of Words and avoid biasing in favour of most frequent words, which can lead to ignoring other meaningful words in a document when using only CountVectorizer.\n",
    "\n",
    "\n",
    "#### 4.3. Select Modeling technique\n",
    "Choosing the right model is a tricky part. One of our data mining goals is to predict categorical customer ratings based on labeled reviews. Therefore, we select a classification algorithm, one of the supervised machine learning algorithms.\n",
    "\n",
    "From the literature review (Andreas and Sarah, 2017, Aurelien, 2019, Medium, 2022):\n",
    "\n",
    "**K-NN classifier** is easy to understand but does particularly badly with datasets with many features and not often used in practice. \n",
    "\n",
    "**Kernelized SVMs** don’t scale very well with the number of samples. Running an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\n",
    "\n",
    "**Decision trees** work well when we have features that are on completely different scales or a mix of binary and continuous features, but this algorithm tends to overfit and provide poor generalization performance.\n",
    "\n",
    "**Random forests** don’t tend to perform well on very high dimensional, sparse data, such as text data. For this kind of data, linear models might be more appropriate.\n",
    "\n",
    "**Naive Bayes classifiers** is faster in training but worse in generalization performance than those linear classifiers like LogisticRegression and LinearSVC. For high-dimensional, sparse data like this, linear models like LogisticRegression and LinearSVC often work best.\n",
    "\n",
    "I decided to select **Linear Support Vector Classification (LinearSVC)** to build the model for predicting rating class based on reviews. Besides, I also use the **LogisticRegression** model as a reference model for performance comparison.\n",
    "\n",
    "#### 4.2. Generate Test Design\n",
    "##### Confusion Matrix\n",
    "Our machine learning problem is a binary classification problem. One of the most comprehensive ways to represent the result of evaluation for binary classification problems is using confusion matrices (Andreas and Sarah, 2017, Aurelien, 2019). \n",
    "\n",
    "Confusion matrices for binary classification is a 2x2 array where rows correspond to the true class (y_test), and the columns correspond to the predicted class (y_pred).\n",
    "\n",
    "##### Accuracy, Precision, Recall, and F1-score\n",
    "- Accuracy is the number of correct predictions divided by the number of all samples = (TN+TP) / (TN+TP+FN+FP)\n",
    "- Precision measures how many of the samples are predicted as positive as actually positive = TP / (TP+FP). If Precision is high, FP has to be small. So, Precision is useful when we want to limit the number of false positives.\n",
    "- Recall, on the other hand, = TP / (TP + FN),  measures how many of the positive samples are captured by the positive predictions. Recall is useful when we need to avoid FN \n",
    "- F-score is the harmonic mean of Precision and recall, is used to provide the full picture = 2(Precision.Recall) / (Precision+Recall)\n",
    "\n",
    "##### Classification Report\n",
    "If we just use the Confusion Matrix or calculate Precision, Recall, and F1-score separately, we need to define which class is positive. However, there is an easier method to get all of these metrics in one shot by using a classification report.\n",
    "\n",
    "***classification_report*** is a library in scikit-learn that allows us to have a comprehensive summary of Precision, Recall, and F1-score.\n",
    "\n",
    "Since our dataset is imbalanced, Precision, Recall, and F1-score are better measuring than accuracy. And we will pay a bit more attention to the minority class, which is a bad rating class (class 0).\n",
    "\n",
    "####  4.3. Build Model\n",
    "**modeling_evaluation** function is built to apply model, run, and evaluate models in one shot. \n",
    "\n",
    "Logistic Regression and Linear Support Vector Classification will be applied to the training set.\n",
    "\n",
    "#### 4.4. Assess Model \n",
    "Our data mining goal is to predict categorical customer ratings based on labeled reviews. Therefore, the model selected is expected to predict the rating class from unseen review data with high accuracy.\n",
    "\n",
    "LinearSVC determines the best decision boundary between vectors of two classes. It can be applied to any kind of vector which encode any kind of data. With tf-idf vectorized text data, LinearSVC will draw the best hyperplane to divide the space into two subspaces. In terms of data mining objectives, the LinearSVC model meets the requirements.\n",
    "\n",
    "The model is applied (fit) to the training set based on reviews and labeled rating class. In other words, the model is trained using the training set, and then the model will be used to predict the rating class for the test set.\n",
    "\n",
    "### Link to detail 04. Modeling and Evaluation Notebook can be found at the end of the next phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6703a",
   "metadata": {},
   "source": [
    "### 5. Evaluation    <a class=\"anchor\" id=\"5-bullet\"></a>\n",
    "#### 5.1. Evaluate the results\n",
    "The classification report for both the training set and test set disclosed that precision, recall, and f1-score are pretty good in both models. However, LinearSVC gives slightly better results than Logistic Regression.\n",
    "\n",
    "Now, we focus on the minority class (class 0 or 'bad' rating class) and pick this class as a positive class in the confusion matrix result of the LinearSVC model. \n",
    "\n",
    "- If we want to predict a review is bad, only if we are very confident, we will use 83% of precision.\n",
    "\n",
    "- If we want to avoid missing too many cases of bad, we will use 88% of recall.\n",
    "\n",
    "There is a trade-off between optimizing precision and recall, and it depends on our purpose, as mentioned above. For instance, if our company does not want to ignore any bad reviews from customers to improve service quality, we will try to optimize the recall metric.\n",
    "\n",
    "A similar interpretation could be made if we pick the good rating class as a positive class in the confusion matrix.\n",
    "\n",
    "In another aspect, If we want to use a balanced metric for both precision and recall to compare the performance between two models, we can use F1-score. In this case, Logistic Regression and LinearSVC has the same F1-score for class 0 (Bad) = 85%.\n",
    "\n",
    "#### 5.2. Review Process \n",
    "As a result of reviewing the whole process of this project, we developed a good interrelation between steps. However, there are several things we could improve: \n",
    "- We could do some more analysis in the Data Understanding phase to get more insights\n",
    "- In the Data Preparation stage, the normalization and lemmatization phase take a long time to complete. In addition to that, we could look in more detail to remove more unmeaningful words\n",
    "- In the Modeling phase, although we got the data mining goal, it seems to be like we haven't explored all the valuable insights from the reviews. This should be done in business understanding again to define new business objectives.\n",
    "\n",
    "#### 5.3. Determine next steps\n",
    "Due to time limitations, we aren't able to do all that we could in this project. The next steps will be determined based on our business objectives and data mining objectives in the future. In particular, this project could do the followings:\n",
    "- Tune the hyperparameter, like changing the C value or algorithms in the model to improve the evaluation metrics.\n",
    "- Use cross-validation and Gridsearch to improve the performance\n",
    "- Try SMOTE to balance the dataset to test the effect on the model\n",
    "- Examine other models and use ROC, AUC to evaluate the performance between them\n",
    "- Build a Neural Networks model for text classification to see how well NNs perform on text data.\n",
    "- ApplyTopic Modeling will be useful for further categorization of new reviews\n",
    "- Build the pipeline to implement the whole process\n",
    "\n",
    "### Link to detail [04. Modeling and Evaluation Notebook](CA_TanThienNguyen_C00278719_04_Modeling&Evaluation.ipynb) \n",
    "(Note: Please remember to run **03.Data Preparation Notebook** first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d846c6",
   "metadata": {},
   "source": [
    "### 6. Discuss and Conclusion <a class=\"anchor\" id=\"6-bullet\"></a>\n",
    "The project performed sentiment analysis using Linear Support Vector Classification (Linear SVC) algorithm to analyse Amazon reviews based on the CRISP-DM model. Business Objectives and Data Mining goals are determined in the Business Understanding phase as a guideline throughout the project. Natural Language Processing (NLP) technique is used to generate the Word Cloud. LinearSVC, a machine learning model, is used to predict categorical rating class, which are good and bad. These data mining goals aim to understand customer feelings about the brands and products through their reviews.\n",
    "\n",
    "In order to achieve these data mining goals, data was first put into the Data Understanding and Data Preparation stages. The Data Understanding phase brought a closer look at the dataset by collecting and describing data. In addition to that, some insights were revealed in Exploratory Data Analysis (EDA) step, and data quality was verified at the end of the phase. Data Preparation is the longest part of the project. It took around 70-80% workload and time. Necessary Features for modeling were extracted and cleaned using Normalization and Lemmatization advanced text processing techniques. Next, the TF-IDF vectorizer was used to transform text data into numerical representation using the Bag of Words strategy so that we could execute our machine learning algorithms. After this phase, data was ready for modeling. \n",
    "\n",
    "K-NN, Decision Trees, Random Forests, Naive Bayes Classifiers, and Kernelized SVMs are good algorithms. However, they don't perform well on very high dimensional, sparse data such as text data. For this kind of data, linear models might be more appropriate. Therefore, Linear Support Vector Classification (LinearSVC), a linear model, was selected together with LogisticRegression as a performance reference for modeling. Confusion Matrix, Precision, Recall, and F1-Score, which were presented in the classification report, are evaluation metrics for the model. The evaluation phase has shown that the classification report for both the training set and test set disclosed that precision, recall, and f1-score are pretty good in both models. However, LinearSVC gives slightly better results than Logistic Regression.\n",
    "\n",
    "Although the project was well organized, we still have many things to improve in the future to make the project quality better. The next steps were also mentioned, such as tuning the hyperparameter, applying Topic Modeling, or using Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146aa5a9",
   "metadata": {},
   "source": [
    "### 7. Bibliography <a class=\"anchor\" id=\"7-bullet\"></a>\n",
    "\n",
    "1. Analytics Vidhya. (2022). Machine Learning [online] Available from: https://www.analyticsvidhya.com/blog/  [accessed 01 March 2022]\n",
    "\n",
    "2. Andreas, C. M. and Sarah G. (2017). Introduction to Machine Learning with Python - A Guide for Data Scientist. 1st ed., California: O’Reilly Media, Inc.\n",
    "\n",
    "3. Aurelien, G. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. 2nd ed., California: O'Reilly Media, Inc.\n",
    "\n",
    "4. DataCamp, Inc. (2022). Machine Learning Tutorials [online] Available from: https://www.datacamp.com/community/tutorials  [accessed 05 March 2022]\n",
    "\n",
    "5. Greg, D. (2022) Data Analytics and Algorithms Module (ANALC5201) Slides, Institute of Technology Carlow, Department of Computing and Networking, unpublised.\n",
    "\n",
    "6. Kaggle. (2022). Machine Learning [online] Available from: https://www.kaggle.com/  [accessed 25 March 2022]\n",
    "\n",
    "7. IBM Corporation. (2011). IBM SPSS Modeler CRISP-DM Guide, New York: IBM Corporation.\n",
    "\n",
    "8. Medium. (2022a). Machine Learning [online] Available from: https://medium.com/tag/machine-learning [accessed 01 March 2022]\n",
    "\n",
    "9. Medium. (2022b). Data Science [online] Available from: https://towardsdatascience.com/  [accessed 01 March 2022]\n",
    "\n",
    "10. NLTK Project. (2022). Machine Learning [online] Available from: https://www.nltk.org/book/  [accessed 05 March 2022]\n",
    "\n",
    "11. Scikit-learn developers. (2022). Machine Learning [online] Available from: https://scikit-learn.org/stable/user_guide.html  [accessed 07 April 2022]\n",
    "\n",
    "12. Stack Exchange Inc. (2022). Machine Learning [online] Available from: https://stackoverflow.com/  [accessed 01 April 2022]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
